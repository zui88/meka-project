%===============================================================================

\chapter{Stand der Technik}\label{Kap:StandDerTechnik}
%08.07.2020
%===============================================================================


\section{RC-Fahrzeug und Fahrbahnmodell}
Die Grundlage zur Umsetzung eines solchen Kollisionsvermeidungsassisten stellt hierbei ein RC-Fahrzeug-Modell im Maßstab 1:12 dar. Dieses kann mittels einer TCP/IP Verbindung gesteuert und programmiert werden. Hierbei wird die Ansteuerung der Hardware und Sensoren von einem Mikrocontroller vorgenommen, welcher über ein Bussystem an einen Raspberry Pi angeschlossen ist. Dieser wird für die Kommunikation über TCP/IP benötigt und realisiert die Regelungsfunktionen und die entsprechenden Fahrmodi. \\
Diese Funktionen und Interfaces sind hierbei in MATLAB, beziehungsweise  SMULINK programmiert und mittels des Hardware-Support-Packages auf die entsprechende Hardware ausgelagert worden. Hierdurch können auch zur Laufzeit wichtige Funktionen oder Zustände in SIMULINK analysiert oder Parameter verändert werden. Um hierbei möglichst vielfältige und realitätsnahe Fahrsituationen realisieren zu können, kann das Modellfahrzeug in Kombination mit einem Fahrbahnmodell verwendet werden. Hierbei werden die Straßenverhältnisse durch ein Laufband mit Fahrbahnmarkierungen simuliert, welches sich in Geschwindigkeit, Steigung und Schräglage beeinflussen lässt. Hierdurch können zusätzlich Fahrbahntrajektorien vorgegeben werden, sodass die Regelungsfunktionen des Fahrzeugs realitätsnah getestet werden können. Es können mehrere Modelle gleichzeitig auf dem Prüfstand fahren, sodass auch Überholmanöver simuliert und durchgeführt werden können. Am Kopfende des Fahrbahnmodells ist zusätzlich ein Bildschirmmonitor angebracht, auf welchem Filmaufnahmen von Straßenfahrten abgespielt werden können und somit ein realistisches Umfeld geschaffen werden kann. Dieser soll zukünftig dazu genutzt werden um Aufnahmen von Fahrten in Kombination mit einer Simulation der Straßenverhältnissen zu nutzen um einen Kollisionsvermeidungsassisten zu implementieren. Hierbei soll das Fahrzeug auf Basis einer Objekterkennung durch ein KNN selbstständig und korrekt auf entsprechende Fahrsituationen reagieren. Hierbei soll beispielsweise bei freiem linken Fahrstreifen und einem Auffahren auf das vorausfahrende Fahrzeug ein Überholvorgang eingeleitet werden. Kann dieser aufgrund anderer Verkehrsteilnehmer nicht ausgeführt werden, muss das Fahrzeug entsprechend verzögert werden. Hierdurch ist ebenfalls eine Fußgängererkennung, beispielsweise an Zebrastreifen möglich, sodass durch Anhalten ein Queren ermöglicht werden muss. Zur Realisierung dieser Funktionen soll nachfolgend ein grundlegendes Framework geschaffen werden, welches es ermöglicht, derartige Detektionsalgorithmen auf das Fahrzeug zu deployen.

\section{Raspberry PI}
\subsection{Target-Hardware}
Die Target-Hardware für ein nachfolgendes Deployment stellt ein Raspberry Pi dar. Hierbei handelt es sich um einen Einplatinencomputer, welcher von der Raspberry Pi Foundation entwickelt wird. Dieser ist je nach verwendeter Hardware schon sehr preisgünstig zu erhalten, da er ursprünglich mit dem Ziel entwickelt wurde, junge Menschen für das Erleben von Programmier- und Hardwarekenntnissen zu begeistern. Da er jedoch zu diesem günstigen Preis viele Schnittstellen wie USB, HDMI, GPIO oder Audioanschlüsse  und für viele Anwendungen ausreichend Rechenleistung zur Verfügung stellt, wurden bereits viele Millionen Geräte verkauft. Das bestehende RC-Modell verwendet einen Raspberry Pi 3B, welcher mit einer ARM Cortex-A53 CPU mit vier Kernen und einer Taktfrequenz von 1200 MHz, sowie einem Arbeitsspeicher von 1GB ausgerüstet ist. Über eine WLAN-Schnittstelle kann dieser remote angesteuert werden. Mittels der vorhandenen GPIO-Pins lassen sich Peripheriegeräte wie Sensoren und Aktoren ansteuern. Durch die Verwendung des Hardware-Support-Packages von MATLAB kann relativ einfach auch umfangreicher Code auf den Pi deployed werden. Das genaue Vorgehen hierbei in Kapitel \ref{Kap:Deploy} noch eingehend erläutert werden.\\
Da angesichts der umfangreichen Rechenoperationen eines KNN mit einer erheblichen Rechenlast gerechnet werden muss, steht zusätzlich noch die aktuellste Version in Form eines Pi 4 zur Verfügung. Dieser verfügt über einen A72 Prozessor mit einer erhöhten Taktfrequenz von 1500 MHz, sowie einen Arbeitsspeicher von vier GB Größe, sodass ausreichend Ressourcen zur Verfügung stehen sollten.   

\subsection{Pi-Cam}
Um diese Hardware nun zur Objekterkennung einsetzen zu können, wird zur Bildakquise eine Kamera als Peripheriegerät benötigt. Die Wahl fiel hierbei auf eine Raspberry Pi-Cam. Diese wurde speziell für den Raspberry entwickelt und kann über die vorhandene CSI-Schnittstelle mittels Flachbandkabel verbunden werden. Hiermit können Bilder mit einer Größe bis zu 2592x1944 Pixel und einer Framerate von 30 Bildern pro Sekunde erfasst werden. Dabei kann zusätzlich auf Aufnahmeparameter wie Fokus, Kontrast und Helligkeit Einfluss genommen werden. Dies ermöglicht insbesondere bei der späteren Implementierung wichtige Möglichkeiten der Anpassung an die jeweils vorherrschenden Licht- und Reflexionsbedingungen. Um die Kamera verwenden zu können, muss die entsprechende Schnittstelle in den Konfigurations-Einstellungen des Pi aktiviert werden. Hernach kann mit folgendem Befehl ein Bild erfasst und somit die Funktionsfähigkeit der Kamera geprüft werden.
\begin{matlabcode}
	raspistill -o testbild.jpg
\end{matlabcode}

\section{Deep Neuronal Networks}
\subsection{Einführung}
Eine Technologie des Maschinellen Lernens stellen die sogenannten Neuronalen Netzwerke dar. Diese versuchen die Struktur und den Aufbau des menschlichen Gehirns nachzuahmen, indem sogenannte künstliche Neuronen in einer Netzstruktur miteinander verbunden werden. Hierdurch soll das zu lösende Problem abstrahiert werden, sodass mithilfe einer universellen Architektur vielerlei Problemstellungen wie Text- oder Bilderkennung lösbar sind. Hierbei soll das Netzwerk auf einen bestimmten Input, wie beispielsweise ein Bild mit einem entsprechenden Output reagieren. Hierzu werden diese Inputdaten als Tensoren auf das erste Layer des Netzwerkes übertragen. Die Aktivierung der jeweils nächsten Schicht ergibt sich hierbei durch die Gewichtung jedes einzelnen Wertes und der jeweiligen Aktivierung der verknüpften Neuronen. Die letzte Schicht des Netzwerkes soll in vorliegendem Fall eine Klassifikation darstellen, mithilfe derer die entsprechenden Objekte eines Bildes klassifiziert und deren Wahrscheinlichkeit ermittelt wird. \newline
Nach der Erstellung eines Netzes sind besagte Gewichte vorerst zufällig verteilt, der Output auf einen jeweiligen Input entspricht daher nicht dem erwarteten Ergebnis. Ziel eines Neuronalen Netzwerkes ist es daher, diese Gewichte selbständig so zu optimieren, dass der Output entsprechend dem Input verändert wird. Vereinfacht ausgedrückt wird hierzu verglichen, was das Netzwerk zu einem entsprechenden Input als Output liefert und dieser Output wird mit dem tatsächlich erwarteten Ergebnis verglichen. Hieraus werden dann die Gewichte eines jeden Layers entsprechend schrittweise angepasst, sodass das Ergebnis schrittweise verbessert wird. Während dieser sog. Forward- und Backpropagation wird eine sog. Costfunction ausgewertet, welche ein Maß für die Genauigkeit des Netzwerkes ist. Mithilfe des Gradient-Descent-Verfahrens wird diese entsprechend minimiert und die neuen Gewichte ermittelt, sodass die Genauigkeit des Netzwerkes gesteigert werden kann.\newline
Hierbei werden je nach Anwendungsgebiet unterschiedlich aufgebaute Netzwerke genutzt. Insbesondere im Bereich der Bildverarbeitung kommen hierbei sogenannte Faltungs-Schichten, die Convolutional-Layer zum Einsatz. Der genaue Aufbau eines Neuronalen Netzwerkes ist hierbei jedoch eine Wissenschaft für sich und hängt stark von der Anwendung und der zur Verfügung stehenden Trainingsdaten ab.
Bei den hier verwendeten Netzen handelt sich um Convolutional-Neural-Networks, sogenannte CNN, welche insbesondere zur Bildverarbeitung eingesetzt werden. Diese sollen im Zuge der Arbeit durch ein sog. YOLOv2-Output-Layer erweitert werden, sodass eine möglichst echtzeitfähige Bildverarbeitung ermöglicht wird. Um möglichst viele verschiedene Objekte mit einem hohen Conficence-Niveau erkennen zu können, werden besonders tiefe, das heißt Netzwerke mit vielen Layern benötigt. Näheres hierzu wird in den folgenden Kapiteln erläutert.
\newpage
\subsection{YOLOV2}
Hierbei wird das CNN im Gegensatz zu bisherigen Ansätzen nicht sequentiell Faltungsoperationen auf verschiedene Bereiche eines Bildes angewandt, wie dies bei klassischen Faltungs-Netzwerken der Fall ist, sondern das Bild wird einmalig mithilfe eines gesamten Netzwerkes analysiert (daher der Name: 'You Only Look Once'). Das Bild wird dabei in Bereiche aufgeteilt, welche entsprechend analysiert werden und deren Ergebnisse überlagert die Vorhersage von klassifizierten Objekten anhand deren Wahrscheinlichkeit und Position ermöglichen. Hierdurch wird im Vergleich zu einer zyklischen Analyse klassischer Netzwerke eine deutlich kürzere Detektionsdauer ermöglicht, sodass sich ein solches Netzwerk sehr gut für eine schnelle Objekterkennung, wie für einen Kollisionsvermeidungsassistenen gefordert, eignet. 
%===============================================================================
% EOF